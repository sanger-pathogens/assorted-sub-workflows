#!/usr/bin/env python3

import argparse
import csv
import json
import logging
import os
import shutil
import sys
from pathlib import Path


def parse_arguments():

    def restricted_float(x):
        try:
            x = float(x)
        except ValueError:
            raise argparse.ArgumentTypeError(f"{x} not a floating-point literal")
        
        if x < 0.0 or x > 100.0:
            raise argparse.ArgumentTypeError(f"{x} not in range [0.0, 100.0]")
        return x

    parser = argparse.ArgumentParser(
        description="Select the best bins based on completeness and contamination thresholds."
    )
    parser.add_argument("bin_summary_tsv", help="TSV summary file describing per-bin quality metrics, e.g. completeness, contamination, N50. For example, generated by CheckM or CheckM2")
    parser.add_argument("bin_dir", help="Directory containing bin FASTA files")
    parser.add_argument("output_dir", help="Directory to save the best bins")
    parser.add_argument("--config", type=Path, help="JSON configuration file to supply a column map between properties and the supplied TSV", required=True)
    parser.add_argument("--min-completeness", type=restricted_float, default=50.0,
                        help="Minimum completeness threshold (default: 50.0)")
    parser.add_argument("--max-contamination", type=restricted_float, default=5.0,
                        help="Maximum contamination threshold (default: 5.0)")
    parser.add_argument("--log", default="INFO", choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
                        help="Logging level (default: INFO)")
    return parser.parse_args()

def setup_logging(level):
    logging.basicConfig(format="%(levelname)s: %(message)s", level=getattr(logging, level))


def parse_column_config(column_config: Path) -> dict:
    with open(column_config, 'r') as f:
        try:
            parsed_config = json.load(f)
        except json.JSONDecodeError as e:
            logging.error(f"Given config file '{column_config}' is malformed:\n{e}")
            sys.exit(1)
    if isinstance(parsed_config, list):
        logging.error(f"Given config file '{column_config}' has an array as the outermost datatype, expected a JSON object. Please remove surrounding '[ ]' in the file.")
        sys.exit(1)
    elif not isinstance(parsed_config, dict):
        logging.error(f"Given config file '{column_config}' appears to only contain a literal. Please ensure the outermost datatype is a JSON object.")
        sys.exit(1)
    return parsed_config


def validate_tsv(tsv_file):
    try: 
        with open(tsv_file, 'r') as f:
            reader = csv.DictReader(f, delimiter='\t')

            if next(reader, None) is None:
                logging.error("TSV file contains no data rows.")
                sys.exit(1)

    except FileNotFoundError as e:
        logging.error(f"TSV file not found: {e}")
        sys.exit(1)

    except Exception as e:
        logging.error(f"Error reading TSV file: {e}")
        sys.exit(1)


def score_bin(completeness, contamination):
    return completeness + 5 * (100 - contamination)

def bin_assessment(bin_base, best_bins, score, style, n50):
    if bin_base not in best_bins or (
        best_bins[bin_base][1] < score or
        (best_bins[bin_base][1] == score and n50 > best_bins[bin_base][2])
    ):
        best_bins[bin_base] = (style, score, n50)
    return best_bins

def select_best_bins(tsv_file, min_compl, max_contam, column_map):
    best_bins = {}
    with open(tsv_file, newline='') as f:
        reader = csv.DictReader(f, delimiter='\t')
        for row in reader:
            completeness = float(row[column_map['completeness']])
            contamination = float(row[column_map['contamination']])

            if completeness < min_compl or contamination > max_contam:
                continue

            bin_full = row[column_map['bin']]  # This is the full bin name e.g long_SRR23994567_bin_1_strict
            bin_parts = bin_full.split('_')
            style = bin_parts[-1]  # This is the bin style e.g strict
            bin_base = '_'.join(bin_parts[:-1]) # This is the base name e.g long_SRR23994567_bin_1

            score = score_bin(completeness, contamination)
            n50 = int(row[column_map['n50']])

            best_bins = bin_assessment(bin_base, best_bins, score, style, n50)

    return best_bins

def copy_best_bins(best_bins, bin_dir, output_dir):
    ''' This function copy the best bins to a destination folder '''
    os.makedirs(output_dir, exist_ok=True)
    copied = 0
    not_found = 0
    for bin_base, (style, _, _) in best_bins.items():
        bin_filename = f"{bin_base}_{style}.fasta"
        bin_path = Path(bin_dir) / bin_filename
        if not bin_path.exists():
            not_found += 1
            logging.warning(f"Bin file not found: {bin_filename}")
            continue
        dest_path = Path(output_dir) / bin_path.name
        shutil.copy(bin_path, dest_path)
        logging.info(f"Copied: {bin_path} -> {dest_path}")
        copied += 1
    logging.info(f"Total bins copied: {copied}, Total bin files not found : {not_found}")

    if len(best_bins.keys())!= copied:
        logging.error(f"Mismatch between number of bins and number of bins copied.\nExpected to copy {len(best_bins.keys())} bins but instead copied {copied}")

def write_filtered_tsv(original_tsv, best_bins, output_tsv, column_map):
    with open(original_tsv, newline='') as infile, open(output_tsv, 'w', newline='') as outfile:
        reader = csv.DictReader(infile, delimiter='\t')
        fieldnames = reader.fieldnames
        writer = csv.DictWriter(outfile, fieldnames=fieldnames, delimiter='\t')
        writer.writeheader()

        for row in reader:
            bin_full = row[column_map['bin']]
            bin_parts = bin_full.split('_')
            style = bin_parts[-1]
            bin_base = '_'.join(bin_parts[:-1])

            if bin_base in best_bins and best_bins[bin_base][0] == style:
                writer.writerow(row)
    logging.info(f"Filtered TSV written to: {output_tsv}")

def main():
    args = parse_arguments()
    setup_logging(args.log)

    validate_tsv(args.bin_summary_tsv)

    logging.info("Parsing config file...")
    column_map = parse_column_config(args.config)

    logging.info("Selecting best bins...")
    best_bins = select_best_bins(args.bin_summary_tsv, args.min_completeness, args.max_contamination, column_map)

    logging.info(f"Found {len(best_bins)} best bins. Copying to {args.output_dir}...")
    copy_best_bins(best_bins, args.bin_dir, args.output_dir)

    filtered_tsv = Path(f"{args.output_dir}_summary.tsv")
    write_filtered_tsv(args.bin_summary_tsv, best_bins, filtered_tsv, column_map)

if __name__ == "__main__":
    main()
